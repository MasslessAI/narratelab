{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "report generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1nU+YpXdR9PwaUOkbYnH7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasslessAI/narratelab/blob/master/report_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIJvCaa7Ihkr",
        "outputId": "79adac5e-b414-43c0-d01f-3b3c3b6d2508"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('gdrive', force_remount=True)\n",
        "\n",
        "root_dir = '/content/gdrive/'\n",
        "\n",
        "gdrive_path = root_dir + 'MyDrive/narratelab/reports'\n",
        "\n",
        "!pip install spacy==3.1.0 sentence-transformers redditcleaner psaw pandas loguru distinctipy colour tqdm praw asyncpraw rake-spacy hdbscan umap-learn==0.5.1\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at gdrive\n",
            "Collecting spacy==3.1.0\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 3.7 MB/s \n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting redditcleaner\n",
            "  Downloading redditcleaner-1.1.2-py3-none-any.whl (4.7 kB)\n",
            "Collecting psaw\n",
            "  Downloading psaw-0.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting loguru\n",
            "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting distinctipy\n",
            "  Downloading distinctipy-1.1.5-py3-none-any.whl (23 kB)\n",
            "Collecting colour\n",
            "  Downloading colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.0)\n",
            "Collecting praw\n",
            "  Downloading praw-7.4.0-py3-none-any.whl (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 71.3 MB/s \n",
            "\u001b[?25hCollecting asyncpraw\n",
            "  Downloading asyncpraw-7.4.0-py3-none-any.whl (172 kB)\n",
            "\u001b[K     |████████████████████████████████| 172 kB 74.2 MB/s \n",
            "\u001b[?25hCollecting rake-spacy\n",
            "  Downloading rake_spacy-0.3.1-py3-none-any.whl (13 kB)\n",
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.27.tar.gz (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 60.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn==0.5.1\n",
            "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (57.4.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.4\n",
            "  Downloading catalogue-2.0.5-py3-none-any.whl (17 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.7\n",
            "  Downloading thinc-8.0.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (621 kB)\n",
            "\u001b[K     |████████████████████████████████| 621 kB 75.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (3.7.4.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 14.1 MB/s \n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.7\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (2.11.3)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 77.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.5.1) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.5.1) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.5.1) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.4.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 80.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn==0.5.1) (0.34.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.1.0) (5.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn==0.5.1) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.1.0) (7.1.2)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 73.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 63.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.4)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 69.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from distinctipy) (3.2.2)\n",
            "Collecting prawcore<3,>=2.1\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting update-checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting aiofiles<=0.6.0\n",
            "  Downloading aiofiles-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting asyncio-extras<=1.3.2\n",
            "  Downloading asyncio_extras-1.3.2-py3-none-any.whl (8.4 kB)\n",
            "Collecting aiosqlite<=0.17.0\n",
            "  Downloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
            "Collecting asyncprawcore<3,>=2.1\n",
            "  Downloading asyncprawcore-2.3.0-py3-none-any.whl (18 kB)\n",
            "Collecting async-generator>=1.3\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting yarl\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 71.4 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 70.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (0.29.24)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 76.5 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->asyncprawcore<3,>=2.1->asyncpraw) (21.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.1.0) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->distinctipy) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->distinctipy) (1.3.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: umap-learn, pynndescent, sentence-transformers, hdbscan\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76564 sha256=b81e509362ab7a9154f58d550831fc8da62d9a13588701db396326d184622fee\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e7/bb/347dc0e510803d7116a13d592b10cc68262da56a8eec4dd72f\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.4-py3-none-any.whl size=52373 sha256=0f400b0008370997a1253358270e0396824eaa0e231a3dfc51378afe8b376d42\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/5b/62/3401692ddad12324249c774c4b15ccb046946021e2b581c043\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-py3-none-any.whl size=126710 sha256=970441f6f09d8018d5b31957f4fc84b595733bcaa63486b443935a490b7128e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c1/0f/faafd427f705c4b012274ba60d9a91d75830306811e1355293\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.27-cp37-cp37m-linux_x86_64.whl size=2311860 sha256=b7f224fcf5431d9abefb5ac2e303a19dd483d7db84d80e25a1fe9253a9cb1dbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/5f/2f/9a259b84003b84847c259779206acecabb25ab56f1506ee72b\n",
            "Successfully built umap-learn pynndescent sentence-transformers hdbscan\n",
            "Installing collected packages: multidict, catalogue, yarl, typer, srsly, pydantic, async-timeout, tokenizers, thinc, spacy-legacy, sacremoses, pyyaml, pathy, huggingface-hub, async-generator, aiohttp, websocket-client, update-checker, transformers, spacy, sentencepiece, pynndescent, prawcore, asyncprawcore, asyncio-extras, aiosqlite, aiofiles, umap-learn, sentence-transformers, redditcleaner, rake-spacy, psaw, praw, loguru, hdbscan, distinctipy, colour, asyncpraw\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed aiofiles-0.6.0 aiohttp-3.7.4.post0 aiosqlite-0.17.0 async-generator-1.10 async-timeout-3.0.1 asyncio-extras-1.3.2 asyncpraw-7.4.0 asyncprawcore-2.3.0 catalogue-2.0.5 colour-0.1.5 distinctipy-1.1.5 hdbscan-0.8.27 huggingface-hub-0.0.12 loguru-0.5.3 multidict-5.1.0 pathy-0.6.0 praw-7.4.0 prawcore-2.3.0 psaw-0.1.0 pydantic-1.8.2 pynndescent-0.5.4 pyyaml-5.4.1 rake-spacy-0.3.1 redditcleaner-1.1.2 sacremoses-0.0.45 sentence-transformers-2.0.0 sentencepiece-0.1.96 spacy-3.1.0 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.8 tokenizers-0.10.3 transformers-4.9.2 typer-0.3.2 umap-learn-0.5.1 update-checker-0.18.0 websocket-client-1.2.1 yarl-1.6.3\n",
            "Collecting en-core-web-trf==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0-py3-none-any.whl (460.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 460.2 MB 8.0 kB/s \n",
            "\u001b[?25hCollecting spacy-transformers<1.1.0,>=1.0.3\n",
            "  Downloading spacy_transformers-1.0.4-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.1.0) (3.1.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (4.62.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (21.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.3.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (8.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (5.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: transformers<4.10.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (4.9.2)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.9.0+cu102)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.8.3-cp37-cp37m-manylinux2014_x86_64.whl (998 kB)\n",
            "\u001b[K     |████████████████████████████████| 998 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (4.6.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (5.4.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.15.0)\n",
            "Installing collected packages: spacy-alignments, spacy-transformers, en-core-web-trf\n",
            "Successfully installed en-core-web-trf-3.1.0 spacy-alignments-0.8.3 spacy-transformers-1.0.4\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20YqRuWgJOeX",
        "outputId": "69c589c4-7dc1-4387-bc47-bdb12fa3cf05"
      },
      "source": [
        "import os\n",
        "path = \"/content\" # /content is pretty much the root. you can choose other path in your colab workspace\n",
        "os.chdir(path)\n",
        "\n",
        "!pip uninstall bertopic --yes\n",
        "!rm -fr BERTopic\n",
        "!git clone https://github.com/MasslessAI/BERTopic.git\n",
        "%cd BERTopic\n",
        "\n",
        "# 2. install the project/module\n",
        "!python setup.py install\n",
        "\n",
        "# 3. Add the project directory to the path\n",
        "import os, sys\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "%cd .."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping bertopic as it is not installed.\u001b[0m\n",
            "Cloning into 'BERTopic'...\n",
            "remote: Enumerating objects: 1125, done.\u001b[K\n",
            "remote: Counting objects: 100% (174/174), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 1125 (delta 111), reused 77 (delta 71), pack-reused 951\u001b[K\n",
            "Receiving objects: 100% (1125/1125), 7.02 MiB | 11.72 MiB/s, done.\n",
            "Resolving deltas: 100% (604/604), done.\n",
            "/content/BERTopic\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating bertopic.egg-info\n",
            "writing bertopic.egg-info/PKG-INFO\n",
            "writing dependency_links to bertopic.egg-info/dependency_links.txt\n",
            "writing requirements to bertopic.egg-info/requires.txt\n",
            "writing top-level names to bertopic.egg-info/top_level.txt\n",
            "writing manifest file 'bertopic.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'bertopic.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/tests\n",
            "copying tests/__init__.py -> build/lib/tests\n",
            "copying tests/test_other.py -> build/lib/tests\n",
            "copying tests/test_topic_representation.py -> build/lib/tests\n",
            "copying tests/conftest.py -> build/lib/tests\n",
            "copying tests/test_bertopic.py -> build/lib/tests\n",
            "copying tests/test_models.py -> build/lib/tests\n",
            "copying tests/test_utils.py -> build/lib/tests\n",
            "creating build/lib/bertopic\n",
            "copying bertopic/_bertopic.py -> build/lib/bertopic\n",
            "copying bertopic/__init__.py -> build/lib/bertopic\n",
            "copying bertopic/_ctfidf.py -> build/lib/bertopic\n",
            "copying bertopic/_utils.py -> build/lib/bertopic\n",
            "copying bertopic/_mmr.py -> build/lib/bertopic\n",
            "creating build/lib/bertopic/plotting\n",
            "copying bertopic/plotting/_topics_over_time.py -> build/lib/bertopic/plotting\n",
            "copying bertopic/plotting/__init__.py -> build/lib/bertopic/plotting\n",
            "copying bertopic/plotting/_distribution.py -> build/lib/bertopic/plotting\n",
            "copying bertopic/plotting/_heatmap.py -> build/lib/bertopic/plotting\n",
            "copying bertopic/plotting/_topics_per_class.py -> build/lib/bertopic/plotting\n",
            "copying bertopic/plotting/_hierarchy.py -> build/lib/bertopic/plotting\n",
            "copying bertopic/plotting/_topics.py -> build/lib/bertopic/plotting\n",
            "copying bertopic/plotting/_barchart.py -> build/lib/bertopic/plotting\n",
            "copying bertopic/plotting/_term_rank.py -> build/lib/bertopic/plotting\n",
            "creating build/lib/bertopic/backend\n",
            "copying bertopic/backend/_base.py -> build/lib/bertopic/backend\n",
            "copying bertopic/backend/__init__.py -> build/lib/bertopic/backend\n",
            "copying bertopic/backend/_word_doc.py -> build/lib/bertopic/backend\n",
            "copying bertopic/backend/_flair.py -> build/lib/bertopic/backend\n",
            "copying bertopic/backend/_gensim.py -> build/lib/bertopic/backend\n",
            "copying bertopic/backend/_utils.py -> build/lib/bertopic/backend\n",
            "copying bertopic/backend/_spacy.py -> build/lib/bertopic/backend\n",
            "copying bertopic/backend/_use.py -> build/lib/bertopic/backend\n",
            "copying bertopic/backend/_sentencetransformers.py -> build/lib/bertopic/backend\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/tests\n",
            "copying build/lib/tests/__init__.py -> build/bdist.linux-x86_64/egg/tests\n",
            "copying build/lib/tests/test_other.py -> build/bdist.linux-x86_64/egg/tests\n",
            "copying build/lib/tests/test_topic_representation.py -> build/bdist.linux-x86_64/egg/tests\n",
            "copying build/lib/tests/conftest.py -> build/bdist.linux-x86_64/egg/tests\n",
            "copying build/lib/tests/test_bertopic.py -> build/bdist.linux-x86_64/egg/tests\n",
            "copying build/lib/tests/test_models.py -> build/bdist.linux-x86_64/egg/tests\n",
            "copying build/lib/tests/test_utils.py -> build/bdist.linux-x86_64/egg/tests\n",
            "creating build/bdist.linux-x86_64/egg/bertopic\n",
            "copying build/lib/bertopic/_bertopic.py -> build/bdist.linux-x86_64/egg/bertopic\n",
            "copying build/lib/bertopic/__init__.py -> build/bdist.linux-x86_64/egg/bertopic\n",
            "creating build/bdist.linux-x86_64/egg/bertopic/plotting\n",
            "copying build/lib/bertopic/plotting/_topics_over_time.py -> build/bdist.linux-x86_64/egg/bertopic/plotting\n",
            "copying build/lib/bertopic/plotting/__init__.py -> build/bdist.linux-x86_64/egg/bertopic/plotting\n",
            "copying build/lib/bertopic/plotting/_distribution.py -> build/bdist.linux-x86_64/egg/bertopic/plotting\n",
            "copying build/lib/bertopic/plotting/_heatmap.py -> build/bdist.linux-x86_64/egg/bertopic/plotting\n",
            "copying build/lib/bertopic/plotting/_topics_per_class.py -> build/bdist.linux-x86_64/egg/bertopic/plotting\n",
            "copying build/lib/bertopic/plotting/_hierarchy.py -> build/bdist.linux-x86_64/egg/bertopic/plotting\n",
            "copying build/lib/bertopic/plotting/_topics.py -> build/bdist.linux-x86_64/egg/bertopic/plotting\n",
            "copying build/lib/bertopic/plotting/_barchart.py -> build/bdist.linux-x86_64/egg/bertopic/plotting\n",
            "copying build/lib/bertopic/plotting/_term_rank.py -> build/bdist.linux-x86_64/egg/bertopic/plotting\n",
            "copying build/lib/bertopic/_ctfidf.py -> build/bdist.linux-x86_64/egg/bertopic\n",
            "copying build/lib/bertopic/_utils.py -> build/bdist.linux-x86_64/egg/bertopic\n",
            "copying build/lib/bertopic/_mmr.py -> build/bdist.linux-x86_64/egg/bertopic\n",
            "creating build/bdist.linux-x86_64/egg/bertopic/backend\n",
            "copying build/lib/bertopic/backend/_base.py -> build/bdist.linux-x86_64/egg/bertopic/backend\n",
            "copying build/lib/bertopic/backend/__init__.py -> build/bdist.linux-x86_64/egg/bertopic/backend\n",
            "copying build/lib/bertopic/backend/_word_doc.py -> build/bdist.linux-x86_64/egg/bertopic/backend\n",
            "copying build/lib/bertopic/backend/_flair.py -> build/bdist.linux-x86_64/egg/bertopic/backend\n",
            "copying build/lib/bertopic/backend/_gensim.py -> build/bdist.linux-x86_64/egg/bertopic/backend\n",
            "copying build/lib/bertopic/backend/_utils.py -> build/bdist.linux-x86_64/egg/bertopic/backend\n",
            "copying build/lib/bertopic/backend/_spacy.py -> build/bdist.linux-x86_64/egg/bertopic/backend\n",
            "copying build/lib/bertopic/backend/_use.py -> build/bdist.linux-x86_64/egg/bertopic/backend\n",
            "copying build/lib/bertopic/backend/_sentencetransformers.py -> build/bdist.linux-x86_64/egg/bertopic/backend\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/test_other.py to test_other.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/test_topic_representation.py to test_topic_representation.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/conftest.py to conftest.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/test_bertopic.py to test_bertopic.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/test_models.py to test_models.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/test_utils.py to test_utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/_bertopic.py to _bertopic.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/plotting/_topics_over_time.py to _topics_over_time.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/plotting/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/plotting/_distribution.py to _distribution.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/plotting/_heatmap.py to _heatmap.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/plotting/_topics_per_class.py to _topics_per_class.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/plotting/_hierarchy.py to _hierarchy.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/plotting/_topics.py to _topics.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/plotting/_barchart.py to _barchart.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/plotting/_term_rank.py to _term_rank.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/_ctfidf.py to _ctfidf.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/_utils.py to _utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/_mmr.py to _mmr.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/backend/_base.py to _base.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/backend/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/backend/_word_doc.py to _word_doc.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/backend/_flair.py to _flair.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/backend/_gensim.py to _gensim.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/backend/_utils.py to _utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/backend/_spacy.py to _spacy.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/backend/_use.py to _use.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bertopic/backend/_sentencetransformers.py to _sentencetransformers.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bertopic.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bertopic.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bertopic.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bertopic.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bertopic.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/bertopic-0.8.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing bertopic-0.8.1-py3.7.egg\n",
            "Copying bertopic-0.8.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding bertopic 0.8.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/bertopic-0.8.1-py3.7.egg\n",
            "Processing dependencies for bertopic==0.8.1\n",
            "Searching for plotly<4.14.3,>=4.7.0\n",
            "Reading https://pypi.org/simple/plotly/\n",
            "Downloading https://files.pythonhosted.org/packages/9d/2e/69579c3db25fa4f85d70a10f8a98d52c2b4a0dcbd153e8f17f425761bef4/plotly-4.14.2-py2.py3-none-any.whl#sha256=955b58509200240b8a97c59194b0a7460cab5b4aa74ef255af60e6d904dda48e\n",
            "Best match: plotly 4.14.2\n",
            "Processing plotly-4.14.2-py2.py3-none-any.whl\n",
            "Installing plotly-4.14.2-py2.py3-none-any.whl to /usr/local/lib/python3.7/dist-packages\n",
            "Adding plotly 4.14.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/plotly-4.14.2-py3.7.egg\n",
            "Searching for numpy>=1.20.0\n",
            "Reading https://pypi.org/simple/numpy/\n",
            "Downloading https://files.pythonhosted.org/packages/8f/3d/7247b219c2c3bc5720a0ae963bf384df1f7f39c17330fcff2bd9d8640768/numpy-1.21.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl#sha256=805459ad8baaf815883d0d6f86e45b3b0b67d823a8f3fa39b1ed9c45eaf5edf1\n",
            "Best match: numpy 1.21.2\n",
            "Processing numpy-1.21.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n",
            "Installing numpy-1.21.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl to /usr/local/lib/python3.7/dist-packages\n",
            "Adding numpy 1.21.2 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/numpy-1.21.2-py3.7-linux-x86_64.egg\n",
            "Searching for sentence-transformers==2.0.0\n",
            "Best match: sentence-transformers 2.0.0\n",
            "Adding sentence-transformers 2.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tqdm==4.62.0\n",
            "Best match: tqdm 4.62.0\n",
            "Adding tqdm 4.62.0 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scikit-learn==0.22.2.post1\n",
            "Best match: scikit-learn 0.22.2.post1\n",
            "Adding scikit-learn 0.22.2.post1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pandas==1.1.5\n",
            "Best match: pandas 1.1.5\n",
            "Adding pandas 1.1.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for umap-learn==0.5.1\n",
            "Best match: umap-learn 0.5.1\n",
            "Adding umap-learn 0.5.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for hdbscan==0.8.27\n",
            "Best match: hdbscan 0.8.27\n",
            "Adding hdbscan 0.8.27 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for retrying==1.3.3\n",
            "Best match: retrying 1.3.3\n",
            "Adding retrying 1.3.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for sentencepiece==0.1.96\n",
            "Best match: sentencepiece 0.1.96\n",
            "Adding sentencepiece 0.1.96 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for torch==1.9.0+cu102\n",
            "Best match: torch 1.9.0+cu102\n",
            "Adding torch 1.9.0+cu102 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for transformers==4.9.2\n",
            "Best match: transformers 4.9.2\n",
            "Adding transformers 4.9.2 to easy-install.pth file\n",
            "Installing transformers-cli script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for torchvision==0.10.0+cu102\n",
            "Best match: torchvision 0.10.0+cu102\n",
            "Adding torchvision 0.10.0+cu102 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for huggingface-hub==0.0.12\n",
            "Best match: huggingface-hub 0.0.12\n",
            "Adding huggingface-hub 0.0.12 to easy-install.pth file\n",
            "Installing huggingface-cli script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for nltk==3.2.5\n",
            "Best match: nltk 3.2.5\n",
            "Adding nltk 3.2.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for joblib==1.0.1\n",
            "Best match: joblib 1.0.1\n",
            "Adding joblib 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for python-dateutil==2.8.2\n",
            "Best match: python-dateutil 2.8.2\n",
            "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numba==0.51.2\n",
            "Best match: numba 0.51.2\n",
            "Adding numba 0.51.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pynndescent==0.5.4\n",
            "Best match: pynndescent 0.5.4\n",
            "Adding pynndescent 0.5.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Cython==0.29.24\n",
            "Best match: Cython 0.29.24\n",
            "Adding Cython 0.29.24 to easy-install.pth file\n",
            "Installing cygdb script to /usr/local/bin\n",
            "Installing cython script to /usr/local/bin\n",
            "Installing cythonize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for typing-extensions==3.7.4.3\n",
            "Best match: typing-extensions 3.7.4.3\n",
            "Adding typing-extensions 3.7.4.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for importlib-metadata==4.6.4\n",
            "Best match: importlib-metadata 4.6.4\n",
            "Adding importlib-metadata 4.6.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for filelock==3.0.12\n",
            "Best match: filelock 3.0.12\n",
            "Adding filelock 3.0.12 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for regex==2019.12.20\n",
            "Best match: regex 2019.12.20\n",
            "Adding regex 2019.12.20 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for packaging==21.0\n",
            "Best match: packaging 21.0\n",
            "Adding packaging 21.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for sacremoses==0.0.45\n",
            "Best match: sacremoses 0.0.45\n",
            "Adding sacremoses 0.0.45 to easy-install.pth file\n",
            "Installing sacremoses script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for PyYAML==5.4.1\n",
            "Best match: PyYAML 5.4.1\n",
            "Adding PyYAML 5.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tokenizers==0.10.3\n",
            "Best match: tokenizers 0.10.3\n",
            "Adding tokenizers 0.10.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for requests==2.23.0\n",
            "Best match: requests 2.23.0\n",
            "Adding requests 2.23.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Pillow==7.1.2\n",
            "Best match: Pillow 7.1.2\n",
            "Adding Pillow 7.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for setuptools==57.4.0\n",
            "Best match: setuptools 57.4.0\n",
            "Adding setuptools 57.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for llvmlite==0.34.0\n",
            "Best match: llvmlite 0.34.0\n",
            "Adding llvmlite 0.34.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for zipp==3.5.0\n",
            "Best match: zipp 3.5.0\n",
            "Adding zipp 3.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyparsing==2.4.7\n",
            "Best match: pyparsing 2.4.7\n",
            "Adding pyparsing 2.4.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for click==7.1.2\n",
            "Best match: click 7.1.2\n",
            "Adding click 7.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for idna==2.10\n",
            "Best match: idna 2.10\n",
            "Adding idna 2.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for certifi==2021.5.30\n",
            "Best match: certifi 2021.5.30\n",
            "Adding certifi 2021.5.30 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for bertopic==0.8.1\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWSulmvWJeFc"
      },
      "source": [
        "import datetime as dt\n",
        "from datetime import date, datetime\n",
        "from loguru import logger\n",
        "import pandas as pd\n",
        "from psaw import PushshiftAPI\n",
        "import time\n",
        "import os\n",
        "import redditcleaner\n",
        "import re\n",
        "import base64\n",
        "import IPython\n",
        "from distinctipy import distinctipy\n",
        "import random\n",
        "from colour import Color\n",
        "import praw\n",
        "\n",
        "\n",
        "# setup parameters\n",
        "start_epoch = int(dt.datetime(2015, 1, 1).timestamp())\n",
        "end_epoch = int(time.time())\n",
        "\n",
        "\n",
        "SUBMISSIONS_FILE = gdrive_path + '/reddit_submission_entrepreneur_2015_01_01_2021_08_17.tsv'\n",
        "#SUBMISSIONS_FILE = None\n",
        "SUBREDDIT = 'entrepreneur'\n",
        "DATA_FILE_NAME = gdrive_path + '/reddit_submission_{}_{}_{}.tsv'.format(\n",
        "    SUBREDDIT, datetime.fromtimestamp(start_epoch).strftime(\"%Y_%m_%d\"),\n",
        "    datetime.fromtimestamp(end_epoch).strftime(\"%Y_%m_%d\"))\n",
        "\n",
        "r = praw.Reddit(\n",
        "  client_id=\"_tkKH5nva25sWQ\",\n",
        "  client_secret=\"cGMI8KyRQlopBTx7vGAEbgCDF6RzwQ\",\n",
        "  user_agent=\"analysis\",\n",
        "  check_for_async=False\n",
        ")\n",
        "\n",
        "\n",
        "api = PushshiftAPI(r)\n",
        "\n",
        "QUESTION_WORDS = [\n",
        "  \"what\",\n",
        "  \"when\",\n",
        "  \"where\",\n",
        "  \"who\",\n",
        "  \"whom\",\n",
        "  \"which\",\n",
        "  \"whose\",\n",
        "  \"why\",\n",
        "  \"how\",\n",
        "  \"wonder\",\n",
        "  \"want\",\n",
        "  \"is anyone\",\n",
        "  \"does anyone\",\n",
        "  \"any tips\",\n",
        "  \"advice\",\n",
        "  \"suggestion\",\n",
        "  \"suggestions\",\n",
        "  \"suggest\",\n",
        "  \"ideas on\",\n",
        "  \"need help\",\n",
        "  \"needs help\",\n",
        "  \"need your help\",\n",
        "  \"serious help\",\n",
        "  \"please help\",\n",
        "  \"challenge\",\n",
        "  \"challenges\",\n",
        "  \"can't stand\",\n",
        "  \"struggle\",\n",
        "  \"struggling\",\n",
        "  \"can't figure out\",\n",
        "  \"help me\",\n",
        "  \"hardest part\",\n",
        "  \"would appreciate\",\n",
        "  \"would really appreciate\",\n",
        "  \"any guidance\",\n",
        "  \"no idea\",\n",
        "  \"confused with\",\n",
        "  \"new to\",\n",
        "  \"is there any way\"\n",
        "]\n",
        "\n",
        "total = 0\n",
        "title_query = '|'.join(map(lambda x: '\"{}\"'.format(x), QUESTION_WORDS))\n",
        "\n",
        "if SUBMISSIONS_FILE is None:\n",
        "  # skip crawling if file already specified\n",
        "  SUBMISSIONS_FILE = DATA_FILE_NAME\n",
        "  while True:\n",
        "      gen = list(\n",
        "          api.search_submissions(\n",
        "              after=start_epoch, \n",
        "              before=end_epoch,\n",
        "              title=title_query,\n",
        "              is_self=True,\n",
        "              is_original_content=True,\n",
        "              subreddit=SUBREDDIT,\n",
        "              # note the num_comments is only updated for the first 24hr\n",
        "              # may need to use praw to get latest meta data\n",
        "              num_comments=\">1\",\n",
        "              filter=['title', 'selftext', 'author', 'permalink', 'num_comments', 'score', 'total_awards_received',\n",
        "                      'upvote_ratio'],\n",
        "              sort='asc', \n",
        "              sort_type='created_utc', \n",
        "              limit=500))\n",
        "\n",
        "      if len(gen) == 0:\n",
        "          break\n",
        "\n",
        "      def submission_filter(submission):\n",
        "          if 'title' not in submission:\n",
        "              return False\n",
        "          if 'selftext' not in submission:\n",
        "              return False\n",
        "          if len(submission['selftext']) == 0:\n",
        "              # if submission is deleted, the psaw returns empty str\n",
        "              return False\n",
        "          if 'author' not in submission:\n",
        "              return False\n",
        "          if submission['author'] == \"[deleted]\":\n",
        "              return False\n",
        "          if any(submission['selftext'] == x for x in [\"[removed]\", \"[deleted]\"]):\n",
        "              return False\n",
        "          return True\n",
        "\n",
        "      def prepare_data(data):\n",
        "          # some of the fields may be missing\n",
        "          # must manually set an init value to avoid\n",
        "          # generating invalid csv\n",
        "          _data = {\n",
        "              'title': '',\n",
        "              'selftext': '',\n",
        "              'author': '',\n",
        "              'permalink': '',\n",
        "              'num_comments': 0,\n",
        "              'score': 0,\n",
        "              'total_awards_received': 0,\n",
        "              'upvote_ratio': 1.0,\n",
        "              'created_utc': None\n",
        "          }\n",
        "\n",
        "          for key in _data:\n",
        "              if key in data and data[key] is not None:\n",
        "                  _data[key] = data[key]\n",
        "\n",
        "          return _data\n",
        "\n",
        "      \n",
        "      items = []\n",
        "      keys = ['title', 'selftext', 'author', 'permalink', 'num_comments', 'score', 'total_awards_received', 'upvote_ratio', 'created_utc']\n",
        "      for _item in gen:\n",
        "        items.append({\n",
        "          'title': _item.title,\n",
        "          'selftext': _item.selftext,\n",
        "          'author': _item.author,\n",
        "          'permalink': _item.permalink,\n",
        "          'num_comments': _item.num_comments,\n",
        "          'score': _item.score,\n",
        "          #'total_awards_received': _item.total_awards_received,\n",
        "          'upvote_ratio': _item.upvote_ratio,\n",
        "          'created_utc': _item.created_utc\n",
        "        })\n",
        "\n",
        "      items = map(prepare_data, items)\n",
        "      # items = map(prepare_data, [item.d_ for item in gen])\n",
        "\n",
        "      items = list(filter(submission_filter, items))\n",
        "      df = pd.DataFrame(items)\n",
        "\n",
        "      # clean data\n",
        "      def clean(text):\n",
        "          text = text.lower()\n",
        "          # remove reddit styles\n",
        "          text = redditcleaner.clean(\n",
        "              text, quote=False, bullet_point=False, link=False, strikethrough=False, spoiler=False, code=False,\n",
        "              superscript=False, table=False)\n",
        "\n",
        "          # refer to https://towardsdatascience.com/cleaning-text-data-with-python-b69b47b97b76\n",
        "          # Remove unicode characters\n",
        "          text = text.encode('ascii', 'ignore').decode()\n",
        "\n",
        "          # Remove Hashtags\n",
        "          text = re.sub(\"#\\S+\", \" \", text)\n",
        "\n",
        "          # Remove markdown links\n",
        "          text = re.sub(r\"\\[(.+)\\]\\(.+\\)\", r\"\\1\", text)\n",
        "\n",
        "          # Remove other urls\n",
        "          text = re.sub(r\"http\\S+\", \" \", text)\n",
        "\n",
        "          # remove text inside brackets\n",
        "          text = re.sub(\"\\(.*?\\)\",\" \", text)\n",
        "          text = re.sub(\"\\[.*?\\]\",\" \", text)\n",
        "\n",
        "          # remove quotes\n",
        "          # remove brackets\n",
        "          # remove semicolon\n",
        "          text = re.sub(r'[\\t()[\\]\\\"*:\\\\]',' ', text)\n",
        "\n",
        "          # remove non-ascii chars\n",
        "          text = re.sub(r\"[^\\x00-\\x7F]+\",' ', text)\n",
        "\n",
        "          # remove x200b zero-width space\n",
        "          text = re.sub(r\"x200b\",'', text)\n",
        "\n",
        "          # Replace the over spaces# if submission is deleted, the psaw returns NaN, must check if \n",
        "              # it's a valid string\n",
        "          text = re.sub('\\s{2,}', \" \", text)\n",
        "\n",
        "          return text\n",
        "\n",
        "      if len(df) > 0:\n",
        "        df['title'] = df['title'].map(clean)\n",
        "        df['selftext'] = df['selftext'].map(clean)\n",
        "\n",
        "        if not os.path.isfile(DATA_FILE_NAME):\n",
        "            df.to_csv(DATA_FILE_NAME, sep='\\t', header='column_names', index=False, quoting=3)\n",
        "        else:  # else it exists so append without writing the header\n",
        "            df.to_csv(DATA_FILE_NAME, sep='\\t', mode='a', header=False, index=False, quoting=3)\n",
        "\n",
        "        start_epoch = int(items[-1]['created_utc'])\n",
        "        total += len(items)\n",
        "\n",
        "        logger.info('Added {} Total {} Last created_utc {}'.format(\n",
        "          len(items), total, date.fromtimestamp(start_epoch)))\n",
        "\n",
        "        time.sleep(3)\n",
        "      else:\n",
        "        break"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYIihP9EK_CI",
        "outputId": "77a3af0e-0df2-4150-cb02-bf6c5771708e"
      },
      "source": [
        "df = pd.read_csv(SUBMISSIONS_FILE, sep='\\t', quoting=3)\n",
        "\n",
        "# data cleanup\n",
        "print('before clean-up # rows: {}'.format(len(df)))\n",
        "cleaned_rows = []\n",
        "\n",
        "AD_INDICATIVE_PHRASES = [\n",
        "  \"your business\", \n",
        "  \"your businesses\", \n",
        "  \"help you\", \n",
        "  \"case study\"\n",
        "  \"how i\",\n",
        "  \"$\",\n",
        "  \"step-by-step\",\n",
        "  \"here is how\",\n",
        "  \"here's how\",\n",
        "  \"part 1\",\n",
        "  \"part 2\",\n",
        "  \"part 3\",\n",
        "  \"ultimate guide\",\n",
        "  \"cheatsheet\",\n",
        "  \"infographic\",\n",
        "  \"ama\"\n",
        "]\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    row_title = row['title'].lower()\n",
        "    if not any(x in ['-', ':', ';'] for x in row_title) and not any(phrase in row_title for phrase in AD_INDICATIVE_PHRASES) and \"?\" in row_title:\n",
        "        '''\n",
        "        1. must contain '?'\n",
        "        2. can only contain alphanumeric, punctuations and space\n",
        "        3. should not contain '-', ':', ';' which indicates ads\n",
        "        '''\n",
        "        cleaned_rows.append(index)\n",
        "\n",
        "df = df[df.index.isin(cleaned_rows)].reset_index()\n",
        "print('after clean-up # rows: {}'.format(len(df)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before clean-up # rows: 31974\n",
            "after clean-up # rows: 18044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZNB4LgxMGbE",
        "outputId": "16dbcb59-f903-4d04-d867-8d34d5d3b1f6"
      },
      "source": [
        "import pprint\n",
        "import spacy\n",
        "from tqdm import tqdm \n",
        "from rake_spacy import Rake\n",
        "\n",
        "spacy.require_gpu()\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "rake = Rake(nlp=nlp)\n",
        "\n",
        "title_cat = []\n",
        "keywords = []\n",
        "lemmatized_combined_text = []\n",
        "\n",
        "# handle cases where selftext is nan\n",
        "df['selftext'] = df['selftext'].map(lambda text: text if type(text) == 'string' else \"\")\n",
        "\n",
        "df['combined_text'] = df['title'] +' '+ df['selftext']\n",
        "for index, row in tqdm(df.iterrows(), position=0, leave=True, total=len(df)):\n",
        "    title_cat.append('NO_WH_WORD')  \n",
        "    for wh_word in QUESTION_WORDS:\n",
        "        if wh_word in row['title'].lower():\n",
        "            title_cat[-1] = wh_word\n",
        "            break\n",
        "\n",
        "df['title_cat'] = title_cat\n",
        "\n",
        "cat_stats = {\n",
        "    'cat': QUESTION_WORDS,\n",
        "    'num_docs': [],\n",
        "    'total_score': [],\n",
        "    'total_comments': []\n",
        "}\n",
        "\n",
        "# print out number of docs per category\n",
        "for wh_word in QUESTION_WORDS:\n",
        "    _df = df[df['title_cat'] == wh_word]\n",
        "    cat_stats['num_docs'].append(len(_df))\n",
        "    cat_stats['total_score'].append(_df['score'].sum())\n",
        "    cat_stats['total_comments'].append(_df['num_comments'].sum())\n",
        "  \n",
        "cat_stats_df = pd.DataFrame(data=cat_stats)\n",
        "\n",
        "print('\\n', cat_stats_df.to_markdown())\n",
        "  "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18044/18044 [00:01<00:00, 10731.20it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " |    | cat                     |   num_docs |   total_score |   total_comments |\n",
            "|---:|:------------------------|-----------:|--------------:|-----------------:|\n",
            "|  0 | what                    |       5993 |         68924 |           101560 |\n",
            "|  1 | when                    |        594 |          6472 |             7637 |\n",
            "|  2 | where                   |       1253 |          8375 |            13967 |\n",
            "|  3 | who                     |        639 |          8352 |             9862 |\n",
            "|  4 | whom                    |          0 |             0 |                0 |\n",
            "|  5 | which                   |        419 |          3238 |             5627 |\n",
            "|  6 | whose                   |          0 |             0 |                0 |\n",
            "|  7 | why                     |        430 |          5568 |             8857 |\n",
            "|  8 | how                     |       6851 |         54049 |            85387 |\n",
            "|  9 | wonder                  |          2 |             6 |               29 |\n",
            "| 10 | want                    |        362 |         10269 |             7192 |\n",
            "| 11 | is anyone               |         63 |          1228 |              916 |\n",
            "| 12 | does anyone             |        350 |          4335 |             3977 |\n",
            "| 13 | any tips                |        119 |           724 |             1230 |\n",
            "| 14 | advice                  |        547 |          3586 |             6033 |\n",
            "| 15 | suggestion              |        153 |           466 |             1387 |\n",
            "| 16 | suggestions             |          0 |             0 |                0 |\n",
            "| 17 | suggest                 |         19 |            18 |              116 |\n",
            "| 18 | ideas on                |          9 |            26 |               75 |\n",
            "| 19 | need help               |         44 |           126 |              464 |\n",
            "| 20 | needs help              |          1 |             0 |               14 |\n",
            "| 21 | need your help          |          3 |            18 |               34 |\n",
            "| 22 | serious help            |          0 |             0 |                0 |\n",
            "| 23 | please help             |         13 |            45 |              111 |\n",
            "| 24 | challenge               |         10 |            47 |               81 |\n",
            "| 25 | challenges              |          0 |             0 |                0 |\n",
            "| 26 | can't stand             |          0 |             0 |                0 |\n",
            "| 27 | struggle                |          4 |             9 |               26 |\n",
            "| 28 | struggling              |         15 |           871 |              350 |\n",
            "| 29 | can't figure out        |          1 |             2 |               10 |\n",
            "| 30 | help me                 |         88 |           455 |              950 |\n",
            "| 31 | hardest part            |          1 |             0 |                7 |\n",
            "| 32 | would appreciate        |          0 |             0 |                0 |\n",
            "| 33 | would really appreciate |          0 |             0 |                0 |\n",
            "| 34 | any guidance            |          0 |             0 |                0 |\n",
            "| 35 | no idea                 |          2 |             3 |               27 |\n",
            "| 36 | confused with           |          0 |             0 |                0 |\n",
            "| 37 | new to                  |          6 |            36 |               66 |\n",
            "| 38 | is there any way        |         27 |            53 |              222 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hMSrzUqSO6z"
      },
      "source": [
        "# !pip uninstall umap-learn -y\n",
        "# !pip install 'umap-learn==0.5.1'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrJqJyXRSA9Z"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/BERTopic')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc9cODxaMY8Y",
        "outputId": "f3f31e92-6dbd-43d1-8eb3-2a7b2813ec79"
      },
      "source": [
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic.backend import BaseEmbedder\n",
        "\n",
        "\n",
        "sentence_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
        "\n",
        "# we must use BaseEmbedder as base class, otherwise BERTopic will use default \n",
        "# SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
        "class CustomSentenceModel(BaseEmbedder):\n",
        "  def __init__(self, embedding_model):\n",
        "    super().__init__()\n",
        "    self.embedding_model = embedding_model\n",
        "  \n",
        "  def embed(self, documents, verbose):\n",
        "    embeddings = self.encode(documents, show_progress_bar=verbose)\n",
        "    return embeddings\n",
        "\n",
        "  def encode(self,documents,show_progress_bar = False):\n",
        "        # need to split phrases into individual words in order to use general\n",
        "        # sentence embedding models\n",
        "\n",
        "        # some doc can be NaN, ignore these\n",
        "        documents = list(map(lambda doc: re.sub('_', ' ', doc), documents))\n",
        "        \n",
        "        embeddings = self.embedding_model.encode(documents, show_progress_bar=show_progress_bar)\n",
        "        return embeddings\n",
        "\n",
        "def preprocess_text(documents_by_cluster):\n",
        "  processed_documents_by_cluster = []\n",
        "  for document in documents_by_cluster:\n",
        "    docs = list(nlp.pipe(document.split('\\t')))\n",
        "    processed_doc = []\n",
        "    for doc in docs:\n",
        "       # merge entities and noun phrases (without DET)\n",
        "      def valid_start_pos(doc, span):\n",
        "        start = span.start\n",
        "        while start < span.end:\n",
        "          if doc[start].pos_ not in ['DET', 'PRON']:\n",
        "            break\n",
        "          start += 1\n",
        "        return start\n",
        "\n",
        "      # remove prefix tokens that are either DET or PRON\n",
        "      # remove spans with less than 2 tokens\n",
        "      # remove spans with 0 tokens after removing prefix\n",
        "      noun_phrase_spans = list(filter(lambda x: x.start < x.end, [doc[valid_start_pos(doc, np_span):np_span.end] for np_span in doc.noun_chunks if np_span.end-np_span.start > 1]))\n",
        "      entities_spans = list(filter(lambda x: x.start < x.end, [doc[valid_start_pos(doc, ent_span):ent_span.end] for ent_span in doc.ents if ent_span.end-ent_span.start > 1]))\n",
        "\n",
        "      spans = spacy.util.filter_spans(noun_phrase_spans+entities_spans)\n",
        "      with doc.retokenize() as retokenizer:\n",
        "        for span in spans:\n",
        "          retokenizer.merge(span)\n",
        "      \n",
        "      processed_text = ' '.join(['_'.join(token.lemma_.split(' ')) for token in doc if not token.is_stop])\n",
        "      processed_doc.append(processed_text)\n",
        "    processed_documents_by_cluster.append('\\t'.join(processed_doc))\n",
        "  return processed_documents_by_cluster\n",
        "\n",
        "df_selection = df[5000:7327].copy().reset_index()\n",
        "_docs = df_selection['combined_text']\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2), max_df=0.95)\n",
        "\n",
        "topic_model = BERTopic(preprocess_text=preprocess_text, \n",
        "                       vectorizer_model=vectorizer,\n",
        "                       embedding_model=CustomSentenceModel(sentence_model),\n",
        "                       nr_topics=\"auto\", calculate_probabilities=True,\n",
        "                       min_topic_size=5)\n",
        "\n",
        "topics, probabilities = topic_model.fit_transform(_docs)\n",
        "topic_info_df = topic_model.get_topic_info()\n",
        "topic_tokens = [[token[0] for token in topic_model.get_topic(topic=row['Topic'])] for idx, row in topic_info_df.iterrows()]\n",
        "topic_info_df['Tokens']=topic_tokens\n",
        "print(topic_info_df[['Topic', 'Count', 'Tokens']].to_markdown())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-18 22:07:30,568 - BERTopic - Transformed documents to Embeddings\n",
            "2021-08-18 22:07:41,694 - BERTopic - Reduced dimensionality with UMAP\n",
            "2021-08-18 22:07:42,982 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
            "2021-08-18 22:08:03,837 - BERTopic - Reduced number of topics from 106 to 68\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "|    |   Topic |   Count | Tokens                                                                                                                                                                                                                                                                                                                                                 |\n",
            "|---:|--------:|--------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "|  0 |      -1 |     720 | ['sell', 'market', 'business', 'want', 'work', 'ask', 'buy', 'want start', 'offer', 'sale']                                                                                                                                                                                                                                                            |\n",
            "|  1 |       0 |     113 | ['successful entrepreneur', 'want entrepreneur', 'entrepreneur job', 'connect entrepreneur', 'entrepreneur successful', 'entrepreneurship entrepreneur', 'try entrepreneur', 'entrepreneur choose', 'entrepreneur think', 'entrepreneur entrepreneur']                                                                                                 |\n",
            "|  2 |       1 |      76 | ['startup', 'startup_idea', 'successful_startup', 'idea startup', 'join startup', 'startup start', 'want start', 'start want', 'startup buy', 'startup check']                                                                                                                                                                                         |\n",
            "|  3 |       2 |      70 | ['steal idea', 'product_idea', 'validate idea', 'app_idea', 'idea product', 'sell idea', 'idea idea', 'tell idea', 'protect idea', 'business_idea']                                                                                                                                                                                                    |\n",
            "|  4 |       3 |      68 | ['start business', 'business start', 'run business', 'small_business', 'start small_business', 'business wish', 'first_business', 'good_business start', 'start own_business', 'multiple_small_business']                                                                                                                                              |\n",
            "|  5 |       4 |      65 | ['blog', 'wordpress', 'feedback', 'improve', 'take feedback', 'host website', 'wordpress_site', 'blogger', 'question ask', 'seo']                                                                                                                                                                                                                      |\n",
            "|  6 |       5 |      62 | ['developer', 'web_developer', 'hire', 'web_design', 'good_graphic_designer', 'concern interest', 'find good_graphic_designer', 'consulting', 'designer', 'advice designer']                                                                                                                                                                           |\n",
            "|  7 |       6 |      59 | ['ship', 'drop_shipping', 'shipping', 'international_shipping', 'handle international_shipping', 'supply_chain', 'advice ship', 'shipping_cost', 'supplier', 'salt_product europe']                                                                                                                                                                    |\n",
            "|  8 |       7 |      51 | ['ecommerce_business', 'sale', 'online_store', 'etsy_store', 'etsy_store notice', 'look sell', 'ebay', 'ecommerce', 'thought pricing', 'store start']                                                                                                                                                                                                  |\n",
            "|  9 |       8 |      46 | ['facebook_ad', 'fb_ad', 'money facebook_ad', 'fb_ad reach', 'promote facebook_ad', 'fb_ad tweak', 'fb_ad work', 'fb_lead_ad', 'fb_advertising', 'fb_advertising other_advertising']                                                                                                                                                                   |\n",
            "| 10 |       9 |      45 | ['freelancer', 'hire', 'freelancer freelancer', 'remote_dev_job hiring_advice', 'work freelance_graphic_designer', 'good_freelancer get', 'unemployed_software_engineer pay_public_speaker', 'find freelance_work', 'find good_freelancer', 'unemployed_software_engineer']                                                                            |\n",
            "| 11 |      10 |      41 | ['book', 'ebook', 'textbook', 'content_writer', 'copywriter', 'content_marketing', 'read', 'course textbook', 'promote writing_service_website', 'fake good_business_book']                                                                                                                                                                            |\n",
            "| 12 |      11 |      40 | ['email_list', 'cold_email', 'send email', 'business_email', '_email_address', '_email_newsletter', '_email_newsletter business_email', 'need email', 'info _email_address', 'general_strategic_advice cold_email']                                                                                                                                    |\n",
            "| 13 |      12 |      36 | ['follower', 'how_many_instagram_follower advertiser', '675k_follower monetize', '675k_follower', 'how_many_instagram_follower', '50k_ghost_follower miss', '50k_ghost_follower', 'accelerate follower_traffic', 'follower_traffic gain', '9k_follower']                                                                                               |\n",
            "| 14 |      13 |      32 | ['online_business', 'difficult start', 'online_marketplace', 'establish reseller_network', 'online_business advice', 'online_bank integrate', 'online_business establish', 'online_business gain', 'online_business purpose', 'online_business question']                                                                                              |\n",
            "| 15 |      14 |      32 | ['money', 'money online', 'money make', 'money money', 'make money', 'kindle_voucher wannabe_entrepreneur', 'ability money', '_strategy money', 'website money', 'money need']                                                                                                                                                                         |\n",
            "| 18 |      15 |      31 | ['fail', 'failure', 'lose', 'fail hate', 'bad_boss', 'fail habit', 'fail go', 'big failure', 'go corporate_world', 'think finally']                                                                                                                                                                                                                    |\n",
            "| 17 |      16 |      31 | ['time skill', 'task', 'focus skill', 'work', 'work hard', 'task goal', 'task get', 'work get', 'work 2_week', 'much_time spare']                                                                                                                                                                                                                      |\n",
            "| 16 |      17 |      31 | ['brand', 'brand_name', 'potential_business_purchase 450_product_tester', 'brand start', 'brand_building', 'rebrande', 'brand_building rebrande', 'brand_name expert', 'brand_name value', 'business product']                                                                                                                                         |\n",
            "| 19 |      18 |      30 | ['social_media', 'social_medium_management_company', 'customer social_medium_management_company', 'run social_media', 'good_practice first_social_medium_post', 'grow blank_social_media_account', 'build social_media_following', 'good_licence_free_image_site social_media_advice', 'next_social_media_giant', 'start social_media_marketing_firm'] |\n",
            "| 20 |      19 |      27 | ['business_partner', 'find business_partner', 'business_partner find', 'know partnership', 'generic_partnership_agreement', 'thought business_partner', 'search partner', 'friend partnership', 'friend business_sense', 'generic_partnership_agreement need']                                                                                         |\n",
            "| 21 |      20 |      25 | ['long', '2016', 'success', '2017', 'achieve long', 'long first_sale', 'achieve 2016', 'slide_deck 2017_goal', '5_year_goal goal', '2016 follow']                                                                                                                                                                                                      |\n",
            "| 22 |      21 |      24 | ['paypal', 'receive payment', 'hold paypal', 'credit debit_card_payment', 'credit_card idea', 'credit_card online_payment_system', 'credit_card withdraw', 'credit_card_payment', 'credit_card_payment good_payment_gateway', 'monthly_bill credit_card']                                                                                              |\n",
            "| 23 |      22 |      23 | ['play business_deal', 'let confront', 'business_deal', 'business_deal make', 'control middle_man', 'deal happen', 'got advice', 'case advice', 'good_advice young_wantrepreneur', 'parent side_hustle']                                                                                                                                               |\n",
            "| 24 |      23 |      23 | ['passive_income', 'income', 'financial_forecast', 'earn passive_income', 'financial_forecast biz_proposal', 'possible_share profit', 'generate passive_income', 'generate income', 'generate forecast', 'loss passive_income']                                                                                                                        |\n",
            "| 27 |      25 |      22 | ['call', 'phone_recycler', 'start phone_recycler', 'phone_recycler money', 'calling', 'start cell_phone_repair_company', 'simple_contact_manager want', 'call business', 'cell_phone_case', 'idea simple_contact_manager']                                                                                                                             |\n",
            "| 28 |      24 |      22 | ['domain', 'domain_name', 'buy domain', 'search domain', 'select co_domain', 'perfect_domain domain_name', 'have com_domain', 'co_domain', 'co_domain instead', 'website_domain']                                                                                                                                                                      |\n",
            "| 26 |      27 |      22 | ['llc', 'form llc', 'incorporate llc', 'datum private_company', 'llc_registration order', 'llc_registration', 'llc_register_agent recommend', 'llc_register_agent', 'start benefit_corporation', 'idea general_business_idea']                                                                                                                         |\n",
            "| 25 |      26 |      22 | ['manufacture', 'industry', 'manufacturing', 'manufacture electronics_product', 'manufacture product', 'manufacture company', 'manufacturing step', 'manufacturing hey', 'start food_manufacturing', 'start commercialize']                                                                                                                            |\n",
            "| 29 |      28 |      20 | ['sell', 'online buy', 'cheap sell', 'custom_flip_flop sell', 'offline sell', 'knowledge_product online', 'online sell', 'allow sell', 'option sell', 'online_side handmade_leather_good_business']                                                                                                                                                    |\n",
            "| 30 |      29 |      20 | ['kickstarter', 'launch campaign', 'event', 'launch', 'invite producthunt', 'festival crowdfunde', 'sponsor event', 'write kickstarter_project', 'show festival', 'local_car_event']                                                                                                                                                                   |\n",
            "| 31 |      30 |      19 | ['youtube', 'remixe_song', 'short_video youtube_ad', 'remixe_song youtube', 'promo_video remixe_song', 'video completely', 'guide youtube_ranking', 'videogame_bar resturaunt', 'crappy_website youtube_video', 's_video good_way']                                                                                                                    |\n",
            "| 32 |      31 |      17 | ['subscription_box_company', 'subscription ensure', 'subscription box', 'work subscription_box_company', 'attn subscription', 'verify subscription_box_service_idea', 'subscription subscription_box_company', 'subscription_box_company start', 'subscription_box_company subscription', 'succeed subscription_box_company']                          |\n",
            "| 33 |      32 |      17 | ['reddit', 'entrepreneur reddit', 'entrepreneur', 'brother launch', 'post reddit', 'successful redditpreneur', 'ask reddit', 'redditpreneur knowledge', 'employee entrepreneur', 'post year_end_profit_']                                                                                                                                              |\n",
            "| 36 |      34 |      16 | ['market_research', 'market_research_tool', 'market_research_tool use', 'market_research_tool choose', 'money market_research_tool', 'market_research market_research_tool', 'market_research determine', 'market_research _tip', 'right_market market_research', 'good_keyword_research_tool day']                                                    |\n",
            "| 34 |      35 |      16 | ['hate', 'marketing hate', 'marketing_campaign advice', 'lack marketing_skill', 'marketing strategy', 'guerrilla_marketing big_marketing_struggle', 'marketing marketing_strategy', 'guerrilla_marketing', 'marketing_strategy favorite_example', 'hate marketing']                                                                                    |\n",
            "| 35 |      33 |      16 | ['monetize', 'monetise', 'successful_fb_meme_page', 'minecraft_gamer monetize', 'successful_fb_meme_page calorie_counter', 'good_way monetize', 'monetize blogginghow', 'feedback monetize', '30k_email_address minecraft_gamer', 'monetize blog']                                                                                                     |\n",
            "| 37 |      36 |      15 | ['mentor', 'contact mentor', 'lucky high_profile_mentor', 'know successful_youtuber', 'advice guy', 'discretely mentor', 'ask mentor', 'accelerator luck', 'successful_youtuber', 'successful_youtuber willing']                                                                                                                                       |\n",
            "| 38 |      37 |      15 | ['affiliate_marketer', 'affiliate_site', 'find affiliate', 'rank top_search', 'affiliate_site profitable', 'affiliate_site affiliate_site', 'affiliate_referral_fee cm', 'affiliate_program know', 'affiliate_program find', 'find affiliate_marketer']                                                                                                |\n",
            "| 39 |      38 |      14 | ['patent_advice', 'patent need', 'patent tread_mark', 'patent_advice provisional', 'patent_advice trademark', 'patent_system', 'patent_system pretty', 'agree patent_system', 'start_to_finish_patent how_much_hire_help', 'start_to_finish_patent']                                                                                                   |\n",
            "| 40 |      39 |      14 | ['alibaba', 'alibaba work', 'alibaba calculate', 'know alibaba', 'excess_alibaba_stock', 'work alibaba_gold_supplier', 'source alibaba', 'order alibaba', 'order alibaba_cif', 'excess_alibaba_stock order']                                                                                                                                           |\n",
            "| 41 |      40 |      14 | ['landing_page', 'look landing_page', 'type landing_page', 'landing_page tell', 'landing_page include', 'landing_page landing_page', 'landing_page make', 'landing_page think', 'build landing_page', 'awesome_image_idea landing_page']                                                                                                               |\n",
            "| 42 |      41 |      13 | ['sale_meeting manage', 'constant_mental_fog hold', 'have anxiety', 'deal loneliness', 'feel depressed', 'anxiety sale_meeting', 'manage stress', 'depressed', 'depressed uncertainty', 'problem constant_mental_fog']                                                                                                                                 |\n",
            "| 43 |      42 |      13 | ['niche', 'niche find', 'competition niche', 'niche niche', 'specific_niche find', 'niche_website niche', 'niche_market', 'example_sale_funnel niche', 'reach niche', 'trade specific_niche']                                                                                                                                                          |\n",
            "| 44 |      43 |      13 | ['shopify', 'shopify_store', 'use shopify', 'online_sale shopify', 'aliexpress_dropshipping_store', 'aliexpress_dropshipping_store reason', 'macbook find', 'make shopify_store', 'way shopify_employee', 'shopify aliexpress_dropshipping_store']                                                                                                     |\n",
            "| 45 |      44 |      12 | ['customer service', 'want customer', 'customer_research notice', 'first_3_customer customer', 'first_3_customer', 'customer_interest first_3_customer', 'customer_interest', 'customer first_100_customer', 'customer pain', 'determine customer_interest']                                                                                           |\n",
            "| 46 |      45 |      11 | ['clothing_line', 'lie fabric', 'find designer', 'client_base brand_new_custom_apparel_and_embroidery_company', 'fabric use', 'come apparel', 'create online_clothing_store', 'sell clothe', 'manufacturer lie', 'help apparel_business']                                                                                                              |\n",
            "| 47 |      46 |      11 | ['prototype', 'start prototype', 'prototype backpack', 'resource prototype', 'product_prototype', 'product_prototype want', 'develop prototype', 'engineer prototype_device', 'prototype app', 'prototype_device']                                                                                                                                     |\n",
            "| 48 |      47 |      10 | ['business quick_logo', 'get future_logo', 'quick_logo', 'think logo', 'hawaii 8_logo', 'future_logo', 'future_logo two_option', 'logo receive', 'logo lot', 'logo change']                                                                                                                                                                            |\n",
            "| 49 |      48 |      10 | ['site s_conversion_rate', 'site s_bounce_rate', 'increase conversion_rate', 'use website_conversion', 'app good_conversion_rate', 'conversion_rate tactic', 'increase user_retention', 'user_retention website', 'user_retention', 'website_conversion ocpm']                                                                                         |\n",
            "| 50 |      49 |      10 | ['zero good_podcast', 'tim_ferriss_podcast think', 'podcast listen', 'podcast guy', 'tim_ferriss_podcast', 'listen podcaster', 'podcast zero', 'podcaster 11_year', 'love tim_ferriss_podcast', 'subscriber podcast']                                                                                                                                  |\n",
            "| 53 |      50 |       9 | ['seed_investment sort', 'financial_projection key_metric', 'grasp financial_projection', 'know stock', 'hard investor', 'sort grasp', 'find right_investor', 'important investor', 'right_investor', 'find investor']                                                                                                                                 |\n",
            "| 52 |      51 |       9 | ['tax', 'tax_return', 'tax_purpose tax_return', 'tax_haven tax', 'tax social_security', 'tax break', 'tax_return taxis', 'tax_work', 'how_much_profit tax', 'require tax_work']                                                                                                                                                                        |\n",
            "| 51 |      52 |       9 | ['contract', 'contract line', 'contractor payment', 'handle contractor', 'handle annual_client_contract', 'sign contract', 'pay handle', 'ritas manage', 'contract seasonal_business', 'draw service_contract']                                                                                                                                        |\n",
            "| 54 |      53 |       8 | ['start new_project', 'stop project', 'stick plan', 'new_project current', 'discipline stick', 'cultivate discipline', 'lead creative_project', 'right project', 'creative_project', 'creative_project job']                                                                                                                                           |\n",
            "| 55 |      54 |       8 | ['advertise', 'good_place advertise', 'people advertise', 'free_and_cheap_place advertise', 'advertise ecommerce_store', 'advertise find', 'advertise free_software', 'advertise local_service_business', 'advertise mobile_application', 'advertise social_medium']                                                                                   |\n",
            "| 56 |      55 |       8 | ['tinder', 'analyze people_taste', 'tinder succeed', 'find popular_product', 'consumer_problem company', 'late_product service', 'consumer_problem', 'whatsapp tinder', 'people_taste', 'people_taste personality']                                                                                                                                    |\n",
            "| 57 |      56 |       8 | ['outsourcing_option', 'evaluate outsourcing_option', 'selling outsource', 'single_thing outsource', 'outsourcing_option advice', 'outsourcing outsource', 'outsourcing ask', 'outsource selling', 'outsource sale', 'outsource choose']                                                                                                               |\n",
            "| 58 |      57 |       7 | ['restaurant sign', 'chargeback_scammer deal', 'fraud_warning order', 'handle fraudulent_product_issue', 'fraudulent_product_issue', 'sign more_than_one_delivery_company', 'deal fraud_warning', 'food_delivery_company restaurant', 'restaurant third_party_delivery_company', 'deal chargeback_scammer']                                            |\n",
            "| 59 |      58 |       7 | ['warehouse_space', 'extra_office', 'everybody s_workspace', 'warehouse_space off_business_season', 'extra_office warehouse_space', 'asian_office operate', 'asian_office', 'plant asian_office', 'operate 500sqm_office_space', 'dedicate_office_space']                                                                                              |\n",
            "| 60 |      59 |       7 | ['business_model', 'business_automation lifestyle_store_business_model', 'business_plan underlie_technology', 'understand business', 'test business_plan', 'lifestyle_store_business_model work', 'lifestyle_store_business_model', 'think business_model', 'business_model think', 'business_model most_innovative_business_model']                   |\n",
            "| 61 |      60 |       7 | ['label_printer', 'label_printer cratejoy', 'regular_printer label_printer', 'print adhesive_label', 'cratejoy', 'user label_printer', '3d_printer print', 'adhesive_label beef_jerky_packaging', 'label_printer use', 'label own_product']                                                                                                            |\n",
            "| 62 |      61 |       7 | ['product obtain', 'offer send', 'ask company', 'company order', 'obtain verbal_offer', 'verbal_offer piece', 'verbal_offer', 'product experience', 'plan sell', 'counter offer']                                                                                                                                                                      |\n",
            "| 63 |      62 |       6 | ['funding crucial_funding', 'structure funding', 'crucial_funding', 'crucial_funding get', 'seek funding', 'funding foundation', 'funding seek', 'funding startup', 'get funding', 'wait funding']                                                                                                                                                     |\n",
            "| 64 |      63 |       6 | ['google form', 'review_type_niche_site tip', 'get survey', 'form survey', 'send survey', 'keyword survey', 'monkey vs', 'survey question', 'survey market_segment', 'survey monkey']                                                                                                                                                                  |\n",
            "| 65 |      64 |       6 | ['paint_striping_business', 'start window_washing_company', 'car_washing_business', 'window_washing_company spring', 'window_washing_company', 'advice paint_striping_business', 'dry_cleaning_business', 'dry_cleaning_business start', 'start dry_cleaning_business', 'labor car_washing_business']                                                  |\n",
            "| 66 |      65 |       5 | ['interesting_idea', 'food_network s_halloween_wars', 'start item', 'idea good_place', 'grandmother knit', 'sell halloween_mask', 'item chainsaw_sculptor', 'knit beautiful_glove', 'advice interesting_idea', 'interesting_idea sock_company']                                                                                                        |\n",
            "| 67 |      66 |       5 | ['sale_plan focus', 'focus continue', 'sale_ability', 'create sale_plan', 'good_place sale_tax_advice', 'continue sale', 'improve sale_ability', 'sale_ability look', 'sale_tax_advice', 'sale_tax_advice create']                                                                                                                                     |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji0s57zBMv7r"
      },
      "source": [
        "# dump topic info\n",
        "REPORT_TOPIC_FILE = SUBMISSIONS_FILE.replace(\"reddit_submission\", \"report_topic\")\n",
        "REPORT_TOPIC_FILE = REPORT_TOPIC_FILE.replace(\".tsv\", \".json\")\n",
        "topic_info_df.to_json(REPORT_TOPIC_FILE)\n",
        "\n",
        "# add topic data to doc then dump\n",
        "doc_topic = []\n",
        "doc_topic_prob = []\n",
        "for i in range(len(topics)):\n",
        "  doc_topic.append(topics[i])\n",
        "  doc_topic_prob.append(probabilities[i][topics[i]])\n",
        "\n",
        "df_selection['topic'] = doc_topic\n",
        "df_selection['topic_prob'] = doc_topic_prob\n",
        "\n",
        "REPORT_SUBMISSION_FILE = SUBMISSIONS_FILE.replace(\"reddit_submission\", \"report_submission\")\n",
        "REPORT_SUBMISSION_FILE = REPORT_SUBMISSION_FILE.replace(\".tsv\", \".json\")\n",
        "df_selection.to_json(REPORT_SUBMISSION_FILE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJFNCtO9ZuCL"
      },
      "source": [
        "# dump topic over time\n",
        "df_topics_over_time = topic_model.topics_over_time(df_selection['title'], topics, df_selection['created_utc'], nr_bins=20)\n",
        "\n",
        "REPORT_TOPIC_OVER_TIME_FILE = SUBMISSIONS_FILE.replace(\"reddit_submission\", \"report_topic_over_time\")\n",
        "REPORT_TOPIC_OVER_TIME_FILE = REPORT_TOPIC_OVER_TIME_FILE.replace(\".tsv\", \".json\")\n",
        "# print(df_topics_over_time.to_markdown())\n",
        "df_topics_over_time.to_json(REPORT_TOPIC_OVER_TIME_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}