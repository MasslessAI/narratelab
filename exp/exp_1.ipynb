{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exp_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7+UaxT49Mzbsop4epFsZf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasslessAI/narratelab/blob/master/exp/exp_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLraSPFiAspw"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BkxwQbbIElv",
        "outputId": "4fa43bb7-f5cf-4b4d-ca77-c4b32c176b78"
      },
      "source": [
        "!pip install spacy keybert[spacy] sentence-transformers redditcleaner psaw pandas loguru\n",
        "# !python -m spacy download en_core_web_trf"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/site-packages (3.1.0)\n",
            "Requirement already satisfied: keybert[spacy] in /usr/local/lib/python3.7/site-packages (0.4.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/site-packages (2.0.0)\n",
            "Requirement already satisfied: redditcleaner in /usr/local/lib/python3.7/site-packages (1.1.2)\n",
            "Requirement already satisfied: psaw in /usr/local/lib/python3.7/site-packages (0.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (1.3.0)\n",
            "Collecting loguru\n",
            "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.5 MB/s \n",
            "\u001b[33mWARNING: keybert 0.4.0 does not provide the extra 'spacy'\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/site-packages (from keybert[spacy]) (1.21.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/site-packages (from keybert[spacy]) (0.24.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.7/site-packages (from keybert[spacy]) (10.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (1.7.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (0.10.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (1.9.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (3.6.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (0.0.8)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (4.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (4.59.0)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from rich>=10.4.0->keybert[spacy]) (0.4.4)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/site-packages (from rich>=10.4.0->keybert[spacy]) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0,>=3.7.4 in /usr/local/lib/python3.7/site-packages (from rich>=10.4.0->keybert[spacy]) (3.10.0.0)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/site-packages (from rich>=10.4.0->keybert[spacy]) (0.9.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn>=0.22.2->keybert[spacy]) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn>=0.22.2->keybert[spacy]) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /root/.local/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (1.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.7.6)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas) (2021.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.7/site-packages (from psaw) (7.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/site-packages (from spacy) (3.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from spacy) (49.6.0.post20210108)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/site-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/site-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/site-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/site-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/site-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /usr/local/lib/python3.7/site-packages (from spacy) (8.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/site-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy) (0.7.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.4->spacy) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/site-packages (from jinja2->spacy) (2.0.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/site-packages (from torchvision->sentence-transformers) (8.3.1)\n",
            "Installing collected packages: loguru\n",
            "Successfully installed loguru-0.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAB4Q2gbTqi7"
      },
      "source": [
        "# Scrap Reddit Data\n",
        "\n",
        "Using pushshift api to quickly search for submissions where title contains wh-words.\n",
        "\n",
        "```\n",
        "what|when|where|who|whom|which|whose|why|how|wonder|want|anyone\n",
        "```\n",
        "\n",
        "Only retrieve submissions that are text submissions, and filter out deleted submissions and submissions whose author is banned.\n",
        "\n",
        "We keep the following fields:\n",
        "\n",
        "* title\n",
        "* selftext\n",
        "* author\n",
        "* permalink\n",
        "* num_comments <font color='blue'>*</font>\n",
        "* score <font color='blue'>*</font>\n",
        "* upvote_ratio <font color='blue'>*</font>\n",
        "* total_awards_received <font color='blue'>*</font>\n",
        "\n",
        "<font color='blue'>*</font> *Used for future ranking purpose*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd5wSUCcU7bx",
        "outputId": "f784e65b-9c25-462a-fbbc-34d47cda2deb"
      },
      "source": [
        "import datetime as dt\n",
        "from datetime import date, datetime\n",
        "from loguru import logger\n",
        "import pandas as pd\n",
        "from psaw import PushshiftAPI\n",
        "import time\n",
        "import os\n",
        "import redditcleaner\n",
        "import re\n",
        "import base64\n",
        "\n",
        "api = PushshiftAPI()\n",
        "\n",
        "start_epoch = int(dt.datetime(2019, 1, 1).timestamp())\n",
        "#end_epoch = int(dt.datetime(2020, 3, 1).timestamp())\n",
        "end_epoch = int(time.time())\n",
        "total = 0\n",
        "\n",
        "SUBREDDIT = 'nft'\n",
        "DATA_FILE_NAME = 'reddit_submission_{}_{}_{}.tsv'.format(\n",
        "    SUBREDDIT, datetime.fromtimestamp(start_epoch).strftime(\"%Y_%m_%d\"),\n",
        "    datetime.fromtimestamp(end_epoch).strftime(\"%Y_%m_%d\"))\n",
        "while True:\n",
        "    gen = list(\n",
        "        api.search_submissions(\n",
        "            after=start_epoch, before=end_epoch,\n",
        "            title=\"what|when|where|who|whom|which|whose|why|how|wonder|want|anyone\", is_self=True,\n",
        "            is_original_content=True, subreddit=SUBREDDIT,\n",
        "            filter=['title', 'selftext', 'author', 'permalink', 'num_comments', 'score', 'total_awards_received',\n",
        "                    'upvote_ratio'],\n",
        "            sort='asc', sort_type='created_utc', limit=500))\n",
        "\n",
        "    if len(gen) == 0:\n",
        "        break\n",
        "\n",
        "    def submission_filter(submission):\n",
        "        if 'title' not in submission:\n",
        "            return False\n",
        "        if 'selftext' not in submission:\n",
        "            return False\n",
        "        if 'author' not in submission:\n",
        "            return False\n",
        "        if submission['author'] == \"[deleted]\":\n",
        "            return False\n",
        "        if any(submission['selftext'] == x for x in [\"[removed]\", \"[deleted]\"]):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def prepare_data(data):\n",
        "        # some of the fields may be missing\n",
        "        # must manually set an init value to avoid\n",
        "        # generating invalid csv\n",
        "        _data = {\n",
        "            'title': '',\n",
        "            'selftext': '',\n",
        "            'author': '',\n",
        "            'permalink': '',\n",
        "            'num_comments': 0,\n",
        "            'score': 0,\n",
        "            'total_awards_received': 0,\n",
        "            'upvote_ratio': 1.0,\n",
        "            'created_utc': None\n",
        "        }\n",
        "\n",
        "        for key in _data:\n",
        "            if key in data and data[key] is not None:\n",
        "                _data[key] = data[key]\n",
        "\n",
        "        return _data\n",
        "\n",
        "\n",
        "    items = map(prepare_data, [item.d_ for item in gen])\n",
        "\n",
        "    items = list(filter(submission_filter, items))\n",
        "\n",
        "    df = pd.DataFrame(items)\n",
        "\n",
        "    # clean data\n",
        "    def clean(text):\n",
        "        # remove reddit styles\n",
        "        text = redditcleaner.clean(\n",
        "            text, quote=False, bullet_point=False, link=False, strikethrough=False, spoiler=False, code=False,\n",
        "            superscript=False, table=False)\n",
        "\n",
        "        # refer to https://towardsdatascience.com/cleaning-text-data-with-python-b69b47b97b76\n",
        "        # Remove unicode characters\n",
        "        text = text.encode('ascii', 'ignore').decode()\n",
        "\n",
        "        # Remove Hashtags\n",
        "        text = re.sub(\"#\\S+\", \" \", text)\n",
        "\n",
        "        # Remove markdown links\n",
        "        text = re.sub(r\"\\[(.+)\\]\\(.+\\)\", r\"\\1\", text)\n",
        "\n",
        "        # Remove other urls\n",
        "        text = re.sub(r\"http\\S+\", \" \", text)\n",
        "\n",
        "        # remove text inside brackets\n",
        "        text = re.sub(\"\\(.*?\\)\",\" \", text)\n",
        "        text = re.sub(\"\\[.*?\\]\",\" \", text)\n",
        "\n",
        "        # remove quotes\n",
        "        # remove brackets\n",
        "        # remove semicolon\n",
        "        text = re.sub(r'[\\t()[\\]\\\"*:\\\\]',' ', text)\n",
        "\n",
        "        # remove non-ascii chars\n",
        "        text = re.sub(r\"[^\\x00-\\x7F]+\",' ', text)\n",
        "\n",
        "         # Replace the over spaces\n",
        "        text = re.sub('\\s{2,}', \" \", text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    df['title'] = df['title'].map(clean)\n",
        "    df['selftext'] = df['selftext'].map(clean)\n",
        "\n",
        "    if not os.path.isfile(DATA_FILE_NAME):\n",
        "        df.to_csv(DATA_FILE_NAME, sep='\\t', header='column_names', index=False, quoting=3)\n",
        "    else:  # else it exists so append without writing the header\n",
        "        df.to_csv(DATA_FILE_NAME, sep='\\t', mode='a', header=False, index=False, quoting=3)\n",
        "\n",
        "    start_epoch = items[-1]['created_utc']\n",
        "    total += len(items)\n",
        "    logger.info('Added {} Total {} Last created_utc {}'.format(\n",
        "        len(items), total, date.fromtimestamp(start_epoch)))\n",
        "\n",
        "    time.sleep(3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 22:56:40.867 | INFO     | __main__:<module>:127 - Added 472 Total 472 Last created_utc 2021-03-13\n",
            "2021-07-08 22:56:45.943 | INFO     | __main__:<module>:127 - Added 330 Total 802 Last created_utc 2021-04-09\n",
            "2021-07-08 22:56:50.728 | INFO     | __main__:<module>:127 - Added 305 Total 1107 Last created_utc 2021-05-17\n",
            "2021-07-08 22:56:55.378 | INFO     | __main__:<module>:127 - Added 281 Total 1388 Last created_utc 2021-07-08\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkJ_ER3vV6gQ"
      },
      "source": [
        "# Load Scrapped Reddit Data\n",
        "\n",
        "Load submissions, filter out ads, and classified them by wh-words\n",
        "\n",
        "From observation, some **submissions are ads**, which often contains the following symbols/phrases\n",
        "\n",
        "```\n",
        "'-', ':', ';' \n",
        "your business(es)\n",
        "help you\n",
        "```\n",
        "\n",
        "<mark>TODO</mark> \n",
        " - [ ] Add more ad-indicative phrases\n",
        " - [ ] Use num_comments to filter ads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AjZF260X1Is"
      },
      "source": [
        "df = pd.read_csv(\"reddit_submission_nft_2019_01_01_2021_07_08.tsv\", sep='\\t', quoting=3)\n",
        "\n",
        "# data cleanup\n",
        "print('before clean-up # rows: {}'.format(len(df)))\n",
        "cleaned_rows = []\n",
        "\n",
        "AD_INDICATIVE_PHRASES = [\"your business\", \"your businesses\", \"help you\"]\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    row_title = row['title'].lower()\n",
        "    if not any(x in ['-', ':', ';'] for x in row_title) and not any(phrase in row_title for phrase in AD_INDICATIVE_PHRASES):\n",
        "        '''\n",
        "        1. must contain '?'\n",
        "        2. can only contain alphanumeric, punctuations and space\n",
        "        3. should not contain '-', ':', ';' which indicates ads\n",
        "        '''\n",
        "        cleaned_rows.append(index)\n",
        "\n",
        "df = df[df.index.isin(cleaned_rows)].reset_index()\n",
        "print('after clean-up # rows: {}'.format(len(df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5lzeN49cbAw"
      },
      "source": [
        "# Classified submissions by wh-words in title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHRjLDsoXq9u"
      },
      "source": [
        "WH_WORD_LIST = [\"what\", \"when\", \"where\", \"who\", \"whom\", \"which\", \"whose\", \"why\", \"how\", \"wonder\", \"want\", \"has anyone\"]\n",
        "\n",
        "title_cat = []\n",
        "for index, row in df.iterrows():\n",
        "    title_cat.append('NO_WH_WORD')\n",
        "    for wh_word in WH_WORD_LIST:\n",
        "        if wh_word in row['title'].lower():\n",
        "            title_cat[-1] = wh_word\n",
        "            break\n",
        "\n",
        "df['title_cat'] = title_cat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn_p6EBAYPd0"
      },
      "source": [
        "# Pick a wh-word category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyw8AKm-YRUv"
      },
      "source": [
        "df_selection = df[df['title_cat'] == 'how'].copy().reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ-0XcmFcrTx"
      },
      "source": [
        "# Clustering submission titles using sentence encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReLmnh9C_BD5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "840126b7-68ab-4806-ac5b-bd22daf899e8"
      },
      "source": [
        "import spacy\n",
        "from keybert import KeyBERT\n",
        "import tensorflow_hub\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_trf\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
        "\n",
        "# kw_model = KeyBERT(model=nlp)\n",
        "#embedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "#kw_model = KeyBERT(model=embedding_model)\n",
        "\n",
        "kw_model = KeyBERT(model=\"paraphrase-MiniLM-L6-v2\")\n",
        "\n",
        "doc = \"\"\"\n",
        "         I've been doing more research on how to sell NFT art and I'm hoping someone can help me out. I want the owner to be able to view the art . Is this possible as an NFT? Or is there a way to attach it to a copy that they can zoom into?\n",
        "      \"\"\"\n",
        "\n",
        "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), highlight=True)\n",
        "print(keywords)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I've been doing more research on how to <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">sell NFT art</span> I'm hoping someone can help me out. I \n",
              "want the owner to be able to view the art . Is this possible as an NFT? Or is there a way to \n",
              "attach it to a copy that they can zoom into?\n",
              "</pre>\n"
            ],
            "text/plain": [
              "I've been doing more research on how to \u001b[30;48;2;255;255;0msell NFT art\u001b[0m I'm hoping someone can help me out. I \n",
              "want the owner to be able to view the art . Is this possible as an NFT? Or is there a way to \n",
              "attach it to a copy that they can zoom into?\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[('sell nft art', 0.7819), ('nft art', 0.7597), ('nft art hoping', 0.739), ('art possible nft', 0.7353), ('sell nft', 0.6575)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiPjaXEeMys-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}